# Research papers I've read recently

> [Attention Is All You Need](https://arxiv.org/abs/1706.03762)

> [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)

> [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)

> [Neural Machine Translation with Byte-Level Subwords](https://arxiv.org/abs/1909.03341)

> [Universal and Transferable Adversarial Attacks on Aligned Language Models](https://arxiv.org/abs/2307.15043)

> [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288)

> [Mistral 7B](https://arxiv.org/pdf/2310.06825.pdf)

> [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781)

> [Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/abs/1310.4546)

> [word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding method
](https://arxiv.org/abs/1402.3722)

> [Large Language Models: A Survey](https://arxiv.org/abs/2402.06196)