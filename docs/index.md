# Research papers I've read recently

> [Deep Unsupervised Learning using Nonequilibrium Thermodynamics](https://arxiv.org/abs/1503.03585)
>> The original paper from 2015 which introduced Diffusion technique to the field of machine learning coming originally from statistical physics

> [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239)
>> Second influential paper for diffusion models published in 2020 which introduced few groundbreaking changes which led to huge jump in quality

> [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
>> Introduced the Transformer architecture which is prevalent in the field of AI now

> [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
>> Google's LLM where the objective isn't next word prediction but masked LM (cloze task) and next sentence prediction

> [Neural Machine Translation with Byte-Level Subwords](https://arxiv.org/abs/1909.03341)
>> Proposed the use of Byte-level Byte Pair Encoding algorithm for tokenization

> [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288)
>> META's LLM with a decoder only architecture

> [Root Mean Square Layer Normalization](https://arxiv.org/abs/1910.07467)
>> Proposed that re-centering invariance in LayerNorm is dispensible and re-scaling invariance alone is enough

> [Self-Attention with Relative Position Representations](https://arxiv.org/abs/1803.02155)
>> Introduced the concept of relative position embeddings

> [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864)
>> Introduced Rotary Positional Embeddings

> [Fast Transformer Decoding: One Write-Head is All You Need](https://arxiv.org/abs/1911.02150)
>> Introduced Multi-Query Attention

> [GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints](https://arxiv.org/abs/2305.13245)
>> Introduced Grouped-Query Attention

> [GLU Variants Improve Transformer](https://arxiv.org/pdf/2002.05202)
>> Proposed using GLU activation functions for Transformers

> [Large Language Models: A Survey](https://arxiv.org/abs/2402.06196)
>> Survery of popular LLM families (GPT, LLaMA, PaLM), popular datasets for LLM training, fine-tuning and evaluation and more

> [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)
>> GPT-3 paper

> [Mistral 7B](https://arxiv.org/pdf/2310.06825)
>> MistralAI's LLM

> [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781)
>> Word2Vec Paper 1

> [Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/abs/1310.4546)
>> Word2Vec Paper 2

> [word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding method](https://arxiv.org/abs/1402.3722)
>> Word2Vec Paper 3