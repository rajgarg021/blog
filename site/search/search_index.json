{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Research papers I've read recently","text":"<p>High-Resolution Image Synthesis with Latent Diffusion Models</p> <p>The key idea proposed in this paper is to model the diffusion process in a lower-dimensional latent space, rather than directly in the high-dimensional pixel space. This latent diffusion model (LDM) approach involves training an autoencoder to map images to a lower-dimensional latent representation. The diffusion process is then performed in this latent space, gradually adding noise to the latent representations and then learning to remove it</p> <p>Deep Unsupervised Learning using Nonequilibrium Thermodynamics</p> <p>The original paper from 2015 which introduced Diffusion technique to the field of machine learning coming originally from statistical physics</p> <p>Denoising Diffusion Probabilistic Models</p> <p>Second influential paper for diffusion models published in 2020 which introduced few groundbreaking changes which led to huge jump in quality</p> <p>Improved Denoising Diffusion Probabilistic Models</p> <p>First paper on diffusion models by OpenAI, finds that learning the variance of the conditional distribution (besides the mean) helps in improving performance</p> <p>Deep Double Descent: Where Bigger Models and More Data Hurt</p> <ul> <li>Describes the double descent phenomenon, shows that it is a function of not just the model size but the number of training epochs as well</li> <li>Introduced a generalized double descent hypothesis: models and training procedures exhibit atypical behavior when their Effective Model Complexity is comparable to the number of train samples</li> <li>Also shows that the double descent phenomenon can lead to a regime where training on more data leads to worse test performance</li> </ul> <p>Attention Is All You Need</p> <p>Introduced the Transformer architecture which is prevalent in the field of AI now</p> <p>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</p> <p>Google's LLM where the objective isn't next word prediction but masked LM (cloze task) and next sentence prediction</p> <p>Neural Machine Translation with Byte-Level Subwords</p> <p>Proposed the use of Byte-level Byte Pair Encoding algorithm for tokenization</p> <p>Llama 2: Open Foundation and Fine-Tuned Chat Models</p> <p>META's LLM with a decoder only architecture</p> <p>Root Mean Square Layer Normalization</p> <p>Proposed that re-centering invariance in LayerNorm is dispensible and re-scaling invariance alone is enough</p> <p>Self-Attention with Relative Position Representations</p> <p>Introduced the concept of relative position embeddings</p> <p>RoFormer: Enhanced Transformer with Rotary Position Embedding</p> <p>Introduced Rotary Positional Embeddings</p> <p>Fast Transformer Decoding: One Write-Head is All You Need</p> <p>Introduced Multi-Query Attention</p> <p>GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints</p> <p>Introduced Grouped-Query Attention</p> <p>GLU Variants Improve Transformer</p> <p>Proposed using GLU activation functions for Transformers</p> <p>Large Language Models: A Survey</p> <p>Survery of popular LLM families (GPT, LLaMA, PaLM), popular datasets for LLM training, fine-tuning and evaluation and more</p> <p>Language Models are Few-Shot Learners</p> <p>GPT-3 paper</p> <p>Mistral 7B</p> <p>MistralAI's LLM</p> <p>Efficient Estimation of Word Representations in Vector Space</p> <p>Word2Vec Paper 1</p> <p>Distributed Representations of Words and Phrases and their Compositionality</p> <p>Word2Vec Paper 2</p> <p>word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding method</p> <p>Word2Vec Paper 3</p>"},{"location":"resume/","title":"Resume","text":""}]}