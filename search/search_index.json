{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Research papers I've read recently","text":"<p>LoRA: Low-Rank Adaptation of Large Language Models</p> <ul> <li>Introduced a method for efficiently fine-tuning LLMs by significantly reducing the number of trainable parameters</li> <li>Key finding was that the delta weights matrix of a large language model has a low intrinsic dimensionality</li> <li>LoRA uses low-rank decomposition to adapt the model's weights, capturing most of the benefits of full fine-tuning</li> <li>Allows for creating small, task-specific adaptations that can be easily switched or combined</li> </ul> <p>Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning</p> <p>The authors show that pre-trained language models have a low intrinsic dimensionality. This means that fine-tuning can be done effectively by optimizing only a small subset of parameters. For instance, they demonstrate that tuning only around 200 parameters can yield 90% of the performance of fine-tuning the full model</p> <p>QLoRA: Efficient Finetuning of Quantized LLMs</p> <ul> <li>QLoRA enables memory-efficient fine-tuning of large language models, such as 65B parameter models, on a single 48GB GPU. It maintains performance similar to full 16-bit precision fine-tuning.</li> <li>Introduced NF4, a new data type optimized for normally distributed weights, helps reduce memory usage by quantizing models to 4-bit precision without significant performance degradation</li> </ul> <p>The case for 4-bit precision: k-bit Inference Scaling Laws</p> <ul> <li>Through extensive experimentation with models ranging from 19M to 176B parameters, the authors demonstrate that 4-bit precision strikes the best balance between model size and performance, particularly for zero-shot tasks</li> <li>Introduced scaling laws that guide how performance (in terms of accuracy and efficiency) scales with model size and bit precision. These laws help determine the bit-precision and model size combinations that maximize performance for zero-shot inference tasks</li> </ul> <p>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</p> <ul> <li>Demonstrated the effectiveness of applying Transformer models to image recognition tasks</li> <li>Vision Transformers (ViT) are trained on sequences of image patches rather than full images, where each patch is treated like a token in a sequence, similar to words in text models</li> </ul> <p>High-Resolution Image Synthesis with Latent Diffusion Models</p> <p>The key idea proposed in this paper is to model the diffusion process in a lower-dimensional latent space, rather than directly in the high-dimensional pixel space. This latent diffusion model (LDM) approach involves training an autoencoder to map images to a lower-dimensional latent representation. The diffusion process is then performed in this latent space, gradually adding noise to the latent representations and then learning to remove it</p> <p>Deep Unsupervised Learning using Nonequilibrium Thermodynamics</p> <p>The original paper from 2015 which introduced Diffusion technique to the field of machine learning coming originally from statistical physics</p> <p>Denoising Diffusion Probabilistic Models</p> <ul> <li>Second influential paper for diffusion models published in 2020 which introduced few groundbreaking changes which led to huge jump in quality</li> <li>The core idea of DDPMs is to model the data distribution as a reverse diffusion process. A diffusion process gradually adds noise to data, effectively destroying its structure, while the reverse process learns to remove this noise, reconstructing the data.</li> </ul> <p>Improved Denoising Diffusion Probabilistic Models</p> <p>First paper on diffusion models by OpenAI, finds that learning the variance of the conditional distribution (besides the mean) helps in improving performance</p> <p>Deep Double Descent: Where Bigger Models and More Data Hurt</p> <ul> <li>Describes the double descent phenomenon, shows that it is a function of not just the model size but the number of training epochs as well</li> <li>Introduced a generalized double descent hypothesis: models and training procedures exhibit atypical behavior when their Effective Model Complexity is comparable to the number of train samples</li> <li>Also shows that the double descent phenomenon can lead to a regime where training on more data leads to worse test performance</li> </ul> <p>Attention Is All You Need</p> <p>Introduced the Transformer architecture which is prevalent in the field of AI now</p> <p>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</p> <p>Google's LLM where the objective isn't next word prediction but masked LM (cloze task) and next sentence prediction</p> <p>Neural Machine Translation with Byte-Level Subwords</p> <p>Proposed the use of Byte-level Byte Pair Encoding algorithm for tokenization</p> <p>Llama 2: Open Foundation and Fine-Tuned Chat Models</p> <p>META's LLM with a decoder only architecture</p> <p>Root Mean Square Layer Normalization</p> <p>Proposed that re-centering invariance in LayerNorm is dispensible and re-scaling invariance alone is enough</p> <p>Self-Attention with Relative Position Representations</p> <p>Introduced the concept of relative position embeddings</p> <p>RoFormer: Enhanced Transformer with Rotary Position Embedding</p> <p>Introduced Rotary Positional Embeddings</p> <p>Fast Transformer Decoding: One Write-Head is All You Need</p> <p>Introduced Multi-Query Attention</p> <p>GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints</p> <p>Introduced Grouped-Query Attention</p> <p>RouteLLM: Learning to Route LLMs with Preference Data</p> <p>GLU Variants Improve Transformer</p> <p>Proposed using GLU activation functions for Transformers</p> <p>Large Language Models: A Survey</p> <p>Survery of popular LLM families (GPT, LLaMA, PaLM), popular datasets for LLM training, fine-tuning and evaluation and more</p> <p>Language Models are Few-Shot Learners</p> <p>GPT-3 paper</p> <p>Mistral 7B</p> <p>MistralAI's LLM</p> <p>Efficient Estimation of Word Representations in Vector Space</p> <p>Word2Vec Paper 1</p> <p>Distributed Representations of Words and Phrases and their Compositionality</p> <p>Word2Vec Paper 2</p> <p>word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding method</p> <p>Word2Vec Paper 3</p>"},{"location":"resume/","title":"My Resume","text":"<p>Your browser doesn't support iframes. You can download the PDF to view it.</p>"}]}